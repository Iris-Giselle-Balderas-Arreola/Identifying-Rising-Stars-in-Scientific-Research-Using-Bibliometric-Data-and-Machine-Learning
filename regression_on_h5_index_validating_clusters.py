# -*- coding: utf-8 -*-
"""Regression on  H5-Index - Validating Clusters.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1os422L_HlCqitgylIMslZCahimhOMpcJ
"""

from google.colab import drive
drive.mount('/content/drive')

#Librerías
import pandas as pd
from sklearn.preprocessing import StandardScaler
import numpy as np
from sklearn.cluster import KMeans
import numpy as np
from sklearn.utils import resample
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import seaborn as sns
from matplotlib import pyplot as plt

df = pd.read_csv('/content/drive/MyDrive/RETORising/metrics_disciplines_authors.csv')

H_df = pd.read_csv('/content/drive/MyDrive/RETORising/main_database.csv' )

# Seleccionar solo las columnas relevantes de H_df_unique (id + HIndices_h5-index_* columnas)
columns_to_add = ['id', 'HIndices_h5-index_2013', 'HIndices_h5-index_2014', 'HIndices_h5-index_2015',
                  'HIndices_h5-index_2016', 'HIndices_h5-index_2017', 'HIndices_h5-index_2018',
                  'HIndices_h5-index_2019', 'HIndices_h5-index_2020', 'HIndices_h5-index_2021',
                  'HIndices_h5-index_2022']

H_df = H_df.drop_duplicates(subset='id', keep='first')
# Asegurar que solo trabajamos con las columnas necesarias
H_df_subset = H_df[columns_to_add]
H_df_subset.fillna(0, inplace=True)  # Reemplazar NaN con 0

# Hacer el merge con df basado en la columna 'id'
df = df.merge(H_df_subset, on='id', how='left')  # 'left' para mantener todas las filas de df

# Suponiendo que tienes un DataFrame llamado df
# Crea el diccionario de mapeo con los números como claves y los nombres correspondientes como valores
column_mapping = {
    '10': 'Multidisciplinary',
    '11': 'Agricultural and Biological Sciences',
    '12': 'Arts and Humanities',
    '13': 'Biochemistry, Genetics and Molecular Biology',
    '14': 'Business, Management and Accounting',
    '15': 'Chemical Engineering',
    '16': 'Chemistry',
    '17': 'Computer Science',
    '18': 'Decision Sciences',
    '19': 'Earth and Planetary Sciences',
    '20': 'Economics, Econometrics and Finance',
    '21': 'Energy',
    '22': 'Engineering',
    '23': 'Environmental Science',
    '24': 'Immunology and Microbiology',
    '25': 'Materials Science',
    '26': 'Mathematics',
    '27': 'Medicine',
    '28': 'Neuroscience',
    '29': 'Nursing',
    '30': 'Pharmacology, Toxicology and Pharmaceutics',
    '31': 'Physics and Astronomy',
    '32': 'Psychology',
    '33': 'Social Sciences',
    '34': 'Veterinary',
    '35': 'Dentistry',
    '36': 'Health Professions'
}

# Renombra las columnas del DataFrame usando el diccionario
df.rename(columns=column_mapping, inplace=True)

df.shape[0]

# Suponiendo que df ya tiene las columnas renombradas con el diccionario column_mapping

# Creamos un diccionario para almacenar los subdataframes
subdataframes = {}

# Iteramos sobre cada columna (disciplina)
for columna in df.columns:
    # Filtramos las filas donde la columna tenga True (o 1)
    subdataframes[columna] = df[df[columna] == True].copy()

# Lista de columnas a eliminar (usando los nombres que se asignaron en el mapeo)
columnas_a_eliminar = list(column_mapping.values())

# Itera sobre cada subdataframe y elimina las columnas especificadas.
for key, subdf in subdataframes.items():
    subdataframes[key] = subdf.drop(columns=columnas_a_eliminar, errors='ignore')

# Ahora, cada subdataframe en el diccionario 'subdataframes' ya no tendrá las columnas indicadas.

# Ejemplo: acceder al subdataframe de 'Psychology'
subdataframes['Medicine']

# Diccionario para almacenar los subdataframes por disciplina y año
subdataframes_por_ano = {}

# Iterar sobre cada disciplina y su subdataframe
for disciplina, subdf in subdataframes.items():
    subdataframes_por_ano[disciplina] = {}  # Inicializamos el diccionario para la disciplina
    # Iterar sobre los años de interés (2013 a 2022)
    for year in range(2013, 2023):
        # Filtrar las columnas que contienen el sufijo '_{year}'
        columns_year = [col for col in subdf.columns if f"_{year}" in col]

        # Solo si se encontraron columnas para el año actual
        if columns_year:
            # Crear el DataFrame con la columna 'id' y las columnas del año actual
            subdataframes_por_ano[disciplina][year] = pd.concat(
                [subdf[['id']], subdf[columns_year]],
                axis=1
            )
        else:
            # Opcional: se puede asignar un DataFrame vacío o simplemente omitir este año
            subdataframes_por_ano[disciplina][year] = pd.DataFrame()

subdataframes_por_ano['Medicine'][2016]

"""# Medicine"""

# Crear un nuevo DataFrame para almacenar los resultados
Medicina_H5R = pd.DataFrame(columns=['id', 'H5_slope'])

# Iterar sobre cada id en el subdataframe 'Medicine'
for author_id in subdataframes['Medicine']['id'].unique():
    # Filtrar el subdataframe para el autor actual
    author_data = subdataframes['Medicine'][subdataframes['Medicine']['id'] == author_id]

    # Obtener los valores del h5-index para los años 2013-2022
    h5_index_values = []
    for year in range(2013, 2023):
        h5_column = f'HIndices_h5-index_{year}'
        if h5_column in author_data.columns:
          h5_index_values.append(author_data[h5_column].iloc[0])
        else:
          h5_index_values.append(np.nan)  # Agregar NaN si la columna no existe

    # Eliminar valores NaN para poder calcular la pendiente
    h5_index_values = [x for x in h5_index_values if not np.isnan(x)]

    if len(h5_index_values) >= 2: #Necesitamos al menos dos puntos para calcular la pendiente
      # Calcular la pendiente de la regresión lineal
      x = np.arange(len(h5_index_values))
      y = np.array(h5_index_values)

      #Usando polyfit para ajustar una línea recta
      slope, _ = np.polyfit(x, y, 1)

      # Agregar el resultado al nuevo DataFrame
      Medicina_H5R = pd.concat([Medicina_H5R, pd.DataFrame({'id': [author_id], 'H5_slope': [slope]})], ignore_index=True)
    else:
      # Agregar el resultado al nuevo DataFrame con pendiente 0 si no hay suficientes datos
      Medicina_H5R = pd.concat([Medicina_H5R, pd.DataFrame({'id': [author_id], 'H5_slope': [0]})], ignore_index=True)

# Mostrar el nuevo DataFrame
Medicina_H5R

# Crear la gráfica de dispersión con cuadrícula
plt.figure(figsize=(10, 6))  # Ajustar el tamaño de la figura (opcional)
plt.scatter(Medicina_H5R['id'], Medicina_H5R['H5_slope'], s=10) # s ajusta el tamaño de los puntos
plt.xlabel('ID')
plt.ylabel('H5_slope')
plt.title('H5_slope vs. ID')
plt.grid(True) # Mostrar la cuadrícula
plt.xticks(rotation=45, ha='right') # Rotar las etiquetas del eje x para mayor legibilidad (opcional)
plt.tight_layout() # Ajustar el diseño para evitar superposiciones (opcional)
plt.show()

# Calcular el 95 de la columna 'H5_slope'
q99 = Medicina_H5R['H5_slope'].quantile(0.99)

# Crear la gráfica de dispersión
plt.figure(figsize=(10, 6))
# Identificar puntos por encima del 95
plt.scatter(Medicina_H5R['id'][Medicina_H5R['H5_slope'] < q99], Medicina_H5R['H5_slope'][Medicina_H5R['H5_slope'] < q99], s=10, label='Below 99th "Percentile"')
plt.scatter(Medicina_H5R['id'][Medicina_H5R['H5_slope'] >= q99], Medicina_H5R['H5_slope'][Medicina_H5R['H5_slope'] >= q99], color='red', s=10, label='Above or Equal to 99th "Percentile"')

plt.axhline(y=q99, color='green', linestyle='--', label=f'99th "Percentile"')

plt.xlabel('ID')
plt.ylabel('H5_slope')
plt.title('H5_slope vs. ID (99th "Percentile" Highlighted)')
plt.grid(True)
plt.xticks(rotation=45, ha='right')
plt.legend()  # Mostrar la leyenda
plt.tight_layout()
plt.show()

# Crear un nuevo DataFrame con los IDs del cuartil 95
Medicina_H5R_q99 = Medicina_H5R[Medicina_H5R['H5_slope'] >= q99][['id']].copy()
Medicina_H5R_q99

# Obtener la lista de IDs
rising_stars_ids = Medicina_H5R_q99['id'].tolist()

# Mostrar la lista de IDs
print(rising_stars_ids)

len(rising_stars_ids)

import pandas as pd
import numpy as np
import statsmodels.api as sm
import matplotlib.pyplot as plt

# Suponiendo que df ya está cargado y contiene los datos

df_original = df.copy()  # Hacer una copia del DataFrame original

# Convertimos el DataFrame a formato largo
id_column = 'id'
df_long = df.melt(id_vars=[id_column], var_name='variable', value_name='value')

# Extraer el año de las variables
# Suponemos que los nombres siguen un patrón como 'FieldWeightedCitationImpact_2013'
df_long['year'] = df_long['variable'].str.extract(r'(\d{4})').astype(float)
df_long['variable'] = df_long['variable'].str.replace(r'_\d{4}', '', regex=True)

# Lista de IDs a considerar
selected_ids = rising_stars_ids # Reemplaza con los IDs que quieras analizar

# Filtrar el DataFrame para incluir solo los IDs seleccionados
df_long = df_long[df_long[id_column].isin(selected_ids)]

# Agrupar por variable y año para obtener la media
df_mean = df_long.groupby(['variable', 'year'])['value'].mean().reset_index()

# Diccionario para almacenar las pendientes de cada variable
slopes = {}

# Modelo de regresión para cada variable
for var in df_mean['variable'].unique():
    df_var = df_mean[df_mean['variable'] == var]

    X = sm.add_constant(df_var['year'])  # Agregar intercepto
    y = df_var['value']
    model = sm.OLS(y, X).fit()

    # Almacenar la pendiente (coeficiente del año)
    slope = model.params[1]
    slopes[var] = slope

    # Predicciones
    df_var['predicted'] = model.predict(X)

    # Visualización
    plt.figure()
    plt.scatter(df_var['year'], df_var['value'], label='Actual')
    plt.plot(df_var['year'], df_var['predicted'], color='red', label='Regresión')
    plt.title(f'Regresión para {var} (Promedio)')
    plt.xlabel('Año')
    plt.ylabel(var)
    plt.legend()
    plt.show()

    print(f"Resumen del modelo para {var} (Promedio):\n", model.summary())

# Convertir las pendientes a un DataFrame para análisis
df_slopes = pd.DataFrame(list(slopes.items()), columns=['Variable', 'Pendiente'])

# Ordenar por velocidad de cambio
df_slopes = df_slopes.sort_values(by='Pendiente', ascending=False)

print("\nVariables con mayor aumento:")
print(df_slopes[df_slopes['Pendiente'] > 0].head())

print("\nVariables con mayor disminución:")
print(df_slopes[df_slopes['Pendiente'] < 0].tail())

Clusters = [56431978500, 26535178500, 16235029000, 56331561100, 35231426700, 55816892300, 57218164219, 14634210200, 7003331101, 56401594400, 35402269600, 24073048100, 35186067500, 57203175853, 55666741300, 7006850485, 7102253367, 55513587000, 26658611900, 7202074046, 6701388352, 56799189700, 7403881797, 7004541381, 12781808200, 7004455625, 7005337290, 57207894092, 55653354700, 57203105612, 55971438800, 7102932049, 56377301200, 34572658900, 7102868950, 57026858200, 55805677400, 55838407000, 55989775200, 7004478068, 35790096500, 7103263092, 55640064500, 35354747000, 7006547963, 55913855100, 7003596797]

# Convertir rising_starsMED['id'] a una lista
rising_stars_ids = Medicina_H5R_q99['id'].tolist()

# Contar cuántos IDs de furritos están en rising_stars_ids
count = sum(id_ in rising_stars_ids for id_ in Clusters)

# Mostrar el resultado
print(f"Hay {count} IDs de rising stars segun los clusters en rising_starsMED.")

# Create a list of all IDs in subdataframes['Medicine']
all_ids = subdataframes['Medicine']['id'].unique()

# Create a list of IDs in rising_stars
rising_stars_ids = Clusters

# Create a list to store the 1/0 values
is_rising_star = []

# Iterate through all IDs and assign 1/0 values
for id_ in all_ids:
    if id_ in rising_stars_ids:
        is_rising_star.append(1)
    else:
        is_rising_star.append(0)

# Create the RS_Medicina dataframe
RS_Medicina = pd.DataFrame({'id': all_ids, 'is_rising_star': is_rising_star})

# Display the dataframe
RS_Medicina

from matplotlib import pyplot as plt
RS_Medicina.plot(kind='scatter', x='id', y='is_rising_star', s=32, alpha=.8)
plt.gca().spines[['top', 'right',]].set_visible(False)

# Unir ambos dataframes en base al ID del investigador
df_medicina = subdataframes['Medicine'].merge(RS_Medicina, on='id', how='inner')
df_medicina

# Separar las muestras con is_rising_star == 0 y == 1
df_majority = df_medicina[df_medicina.is_rising_star==0]
df_minority = df_medicina[df_medicina.is_rising_star==1]

# Hacer undersampling en la clase mayoritaria
df_majority_downsampled = resample(df_majority,
                                 replace=False,    # sample without replacement
                                 n_samples=len(df_minority),     # to match minority class
                                 random_state=123) # reproducible results

# Combinar las muestras minoritarias con las mayoritarias downsampled
df_medicina = pd.concat([df_majority_downsampled, df_minority])

# Mostrar el nuevo dataframe
df_medicina

# Separar las características (X) y las etiquetas (y)
X = df_medicina.drop(columns=['id', 'is_rising_star'])  # Eliminamos 'id' y 'is_rising_star' de las características
y = df_medicina['is_rising_star']  # Etiqueta

# Dividir en entrenamiento y prueba
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)

# Crear el clasificador Random Forest
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)

# Entrenar el modelo
rf_model.fit(X_train, y_train)

# Hacer predicciones en el conjunto de prueba
y_pred = rf_model.predict(X_test)

# Calcular la exactitud
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy: {accuracy}')

# Mostrar reporte de clasificación (precisión, recall, F1-score)
print('Classification Report:')
print(classification_report(y_test, y_pred))

# Calcular la matriz de confusión
cm = confusion_matrix(y_test, y_pred)

# Crear un mapa de calor con seaborn
plt.figure(figsize=(6, 4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['No Rising Star', 'Rising Star'], yticklabels=['No Rising Star', 'Rising Star'], cbar=False)

# Agregar etiquetas y título
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')

# Mostrar la matriz
plt.show()

"""# Agricultural and Biological Sciences"""

# Crear un nuevo DataFrame para almacenar los resultados
ABS_H5R = pd.DataFrame(columns=['id', 'H5_slope'])

# Iterar sobre cada id en el subdataframe 'Agricultural and Biological Sciences'
for author_id in subdataframes['Agricultural and Biological Sciences']['id'].unique():
    # Filtrar el subdataframe para el autor actual
    author_data = subdataframes['Agricultural and Biological Sciences'][subdataframes['Agricultural and Biological Sciences']['id'] == author_id]

    # Obtener los valores del h5-index para los años 2013-2022
    h5_index_values = []
    for year in range(2013, 2023):
        h5_column = f'HIndices_h5-index_{year}'
        if h5_column in author_data.columns:
          h5_index_values.append(author_data[h5_column].iloc[0])
        else:
          h5_index_values.append(np.nan)  # Agregar NaN si la columna no existe

    # Eliminar valores NaN para poder calcular la pendiente
    h5_index_values = [x for x in h5_index_values if not np.isnan(x)]

    if len(h5_index_values) >= 2: #Necesitamos al menos dos puntos para calcular la pendiente
      # Calcular la pendiente de la regresión lineal
      x = np.arange(len(h5_index_values))
      y = np.array(h5_index_values)

      #Usando polyfit para ajustar una línea recta
      slope, _ = np.polyfit(x, y, 1)

      # Agregar el resultado al nuevo DataFrame
      ABS_H5R = pd.concat([ABS_H5R, pd.DataFrame({'id': [author_id], 'H5_slope': [slope]})], ignore_index=True)
    else:
      # Agregar el resultado al nuevo DataFrame con pendiente 0 si no hay suficientes datos
      ABS_H5R = pd.concat([ABS_H5R, pd.DataFrame({'id': [author_id], 'H5_slope': [0]})], ignore_index=True)

# Mostrar el nuevo DataFrame
ABS_H5R

# Crear la gráfica de dispersión con cuadrícula
plt.figure(figsize=(10, 6))  # Ajustar el tamaño de la figura (opcional)
plt.scatter(ABS_H5R['id'], ABS_H5R['H5_slope'], s=10) # s ajusta el tamaño de los puntos
plt.xlabel('ID')
plt.ylabel('H5_slope')
plt.title('H5_slope vs. ID')
plt.grid(True) # Mostrar la cuadrícula
plt.xticks(rotation=45, ha='right') # Rotar las etiquetas del eje x para mayor legibilidad (opcional)
plt.tight_layout() # Ajustar el diseño para evitar superposiciones (opcional)
plt.show()

# Calcular el 95 de la columna 'H5_slope'
q99 = ABS_H5R['H5_slope'].quantile(0.99)

# Crear la gráfica de dispersión
plt.figure(figsize=(10, 6))
# Identificar puntos por encima del 95
plt.scatter(ABS_H5R['id'][ABS_H5R['H5_slope'] < q99], ABS_H5R['H5_slope'][ABS_H5R['H5_slope'] < q99], s=10, label='Below 99th "Percentile"')
plt.scatter(ABS_H5R['id'][ABS_H5R['H5_slope'] >= q99], ABS_H5R['H5_slope'][ABS_H5R['H5_slope'] >= q99], color='red', s=10, label='Above or Equal to 99th "Percentile"')

plt.axhline(y=q99, color='green', linestyle='--', label=f'99th "Percentile"')

plt.xlabel('ID')
plt.ylabel('H5_slope')
plt.title('H5_slope vs. ID (99th "Percentile" Highlighted)')
plt.grid(True)
plt.xticks(rotation=45, ha='right')
plt.legend()  # Mostrar la leyenda
plt.tight_layout()
plt.show()

# Crear un nuevo DataFrame con los IDs del cuartil 95
ABS_H5R_q99 = ABS_H5R[ABS_H5R['H5_slope'] >= q99][['id']].copy()
ABS_H5R_q99

# Obtener la lista de IDs
rising_stars_ids = ABS_H5R_q99['id'].tolist()

# Mostrar la lista de IDs
print(rising_stars_ids)

len(rising_stars_ids)

import pandas as pd
import numpy as np
import statsmodels.api as sm
import matplotlib.pyplot as plt

# Suponiendo que df ya está cargado y contiene los datos

df_original = df.copy()  # Hacer una copia del DataFrame original

# Convertimos el DataFrame a formato largo
id_column = 'id'
df_long = df.melt(id_vars=[id_column], var_name='variable', value_name='value')

# Extraer el año de las variables
# Suponemos que los nombres siguen un patrón como 'FieldWeightedCitationImpact_2013'
df_long['year'] = df_long['variable'].str.extract(r'(\d{4})').astype(float)
df_long['variable'] = df_long['variable'].str.replace(r'_\d{4}', '', regex=True)

# Lista de IDs a considerar
selected_ids = rising_stars_ids # Reemplaza con los IDs que quieras analizar

# Filtrar el DataFrame para incluir solo los IDs seleccionados
df_long = df_long[df_long[id_column].isin(selected_ids)]

# Agrupar por variable y año para obtener la media
df_mean = df_long.groupby(['variable', 'year'])['value'].mean().reset_index()

# Diccionario para almacenar las pendientes de cada variable
slopes = {}

# Modelo de regresión para cada variable
for var in df_mean['variable'].unique():
    df_var = df_mean[df_mean['variable'] == var]

    X = sm.add_constant(df_var['year'])  # Agregar intercepto
    y = df_var['value']
    model = sm.OLS(y, X).fit()

    # Almacenar la pendiente (coeficiente del año)
    slope = model.params[1]
    slopes[var] = slope

    # Predicciones
    df_var['predicted'] = model.predict(X)

    # Visualización
    plt.figure()
    plt.scatter(df_var['year'], df_var['value'], label='Actual')
    plt.plot(df_var['year'], df_var['predicted'], color='red', label='Regresión')
    plt.title(f'Regresión para {var} (Promedio)')
    plt.xlabel('Año')
    plt.ylabel(var)
    plt.legend()
    plt.show()

    print(f"Resumen del modelo para {var} (Promedio):\n", model.summary())

# Convertir las pendientes a un DataFrame para análisis
df_slopes = pd.DataFrame(list(slopes.items()), columns=['Variable', 'Pendiente'])

# Ordenar por velocidad de cambio
df_slopes = df_slopes.sort_values(by='Pendiente', ascending=False)

print("\nVariables con mayor aumento:")
print(df_slopes[df_slopes['Pendiente'] > 0].head())

print("\nVariables con mayor disminución:")
print(df_slopes[df_slopes['Pendiente'] < 0].tail())

"""# Biochemistry, Genetics and Molecular Biology"""

# Crear un nuevo DataFrame para almacenar los resultados
GMB_H5R = pd.DataFrame(columns=['id', 'H5_slope'])

# Iterar sobre cada id en el subdataframe 'Biochemistry, Genetics and Molecular Biology'
for author_id in subdataframes['Biochemistry, Genetics and Molecular Biology']['id'].unique():
    # Filtrar el subdataframe para el autor actual
    author_data = subdataframes['Biochemistry, Genetics and Molecular Biology'][subdataframes['Biochemistry, Genetics and Molecular Biology']['id'] == author_id]

    # Obtener los valores del h5-index para los años 2013-2022
    h5_index_values = []
    for year in range(2013, 2023):
        h5_column = f'HIndices_h5-index_{year}'
        if h5_column in author_data.columns:
          h5_index_values.append(author_data[h5_column].iloc[0])
        else:
          h5_index_values.append(np.nan)  # Agregar NaN si la columna no existe

    # Eliminar valores NaN para poder calcular la pendiente
    h5_index_values = [x for x in h5_index_values if not np.isnan(x)]

    if len(h5_index_values) >= 2: #Necesitamos al menos dos puntos para calcular la pendiente
      # Calcular la pendiente de la regresión lineal
      x = np.arange(len(h5_index_values))
      y = np.array(h5_index_values)

      #Usando polyfit para ajustar una línea recta
      slope, _ = np.polyfit(x, y, 1)

      # Agregar el resultado al nuevo DataFrame
      GMB_H5R = pd.concat([GMB_H5R, pd.DataFrame({'id': [author_id], 'H5_slope': [slope]})], ignore_index=True)
    else:
      # Agregar el resultado al nuevo DataFrame con pendiente 0 si no hay suficientes datos
      GMB_H5R = pd.concat([GMB_H5R, pd.DataFrame({'id': [author_id], 'H5_slope': [0]})], ignore_index=True)

# Mostrar el nuevo DataFrame
GMB_H5R

# Crear la gráfica de dispersión con cuadrícula
plt.figure(figsize=(10, 6))  # Ajustar el tamaño de la figura (opcional)
plt.scatter(GMB_H5R['id'], GMB_H5R['H5_slope'], s=10) # s ajusta el tamaño de los puntos
plt.xlabel('ID')
plt.ylabel('H5_slope')
plt.title('H5_slope vs. ID')
plt.grid(True) # Mostrar la cuadrícula
plt.xticks(rotation=45, ha='right') # Rotar las etiquetas del eje x para mayor legibilidad (opcional)
plt.tight_layout() # Ajustar el diseño para evitar superposiciones (opcional)
plt.show()

# Calcular el 95 de la columna 'H5_slope'
q99 = GMB_H5R['H5_slope'].quantile(0.99)

# Crear la gráfica de dispersión
plt.figure(figsize=(10, 6))
# Identificar puntos por encima del 95
plt.scatter(GMB_H5R['id'][GMB_H5R['H5_slope'] < q99], GMB_H5R['H5_slope'][GMB_H5R['H5_slope'] < q99], s=10, label='Below 99th "Percentile"')
plt.scatter(GMB_H5R['id'][GMB_H5R['H5_slope'] >= q99], GMB_H5R['H5_slope'][GMB_H5R['H5_slope'] >= q99], color='red', s=10, label='Above or Equal to 99th "Percentile"')

plt.axhline(y=q99, color='green', linestyle='--', label=f'99th "Percentile"')

plt.xlabel('ID')
plt.ylabel('H5_slope')
plt.title('H5_slope vs. ID (99th "Percentile" Highlighted)')
plt.grid(True)
plt.xticks(rotation=45, ha='right')
plt.legend()  # Mostrar la leyenda
plt.tight_layout()
plt.show()

# Crear un nuevo DataFrame con los IDs del cuartil 95
GMB_H5R_q99 = GMB_H5R[GMB_H5R['H5_slope'] >= q99][['id']].copy()
GMB_H5R_q99

# Obtener la lista de IDs
rising_stars_ids = GMB_H5R_q99['id'].tolist()

# Mostrar la lista de IDs
print(rising_stars_ids)

len(rising_stars_ids)

import pandas as pd
import numpy as np
import statsmodels.api as sm
import matplotlib.pyplot as plt

# Suponiendo que df ya está cargado y contiene los datos

df_original = df.copy()  # Hacer una copia del DataFrame original

# Convertimos el DataFrame a formato largo
id_column = 'id'
df_long = df.melt(id_vars=[id_column], var_name='variable', value_name='value')

# Extraer el año de las variables
# Suponemos que los nombres siguen un patrón como 'FieldWeightedCitationImpact_2013'
df_long['year'] = df_long['variable'].str.extract(r'(\d{4})').astype(float)
df_long['variable'] = df_long['variable'].str.replace(r'_\d{4}', '', regex=True)

# Lista de IDs a considerar
selected_ids = rising_stars_ids # Reemplaza con los IDs que quieras analizar

# Filtrar el DataFrame para incluir solo los IDs seleccionados
df_long = df_long[df_long[id_column].isin(selected_ids)]

# Agrupar por variable y año para obtener la media
df_mean = df_long.groupby(['variable', 'year'])['value'].mean().reset_index()

# Diccionario para almacenar las pendientes de cada variable
slopes = {}

# Modelo de regresión para cada variable
for var in df_mean['variable'].unique():
    df_var = df_mean[df_mean['variable'] == var]

    X = sm.add_constant(df_var['year'])  # Agregar intercepto
    y = df_var['value']
    model = sm.OLS(y, X).fit()

    # Almacenar la pendiente (coeficiente del año)
    slope = model.params[1]
    slopes[var] = slope

    # Predicciones
    df_var['predicted'] = model.predict(X)

    # Visualización
    plt.figure()
    plt.scatter(df_var['year'], df_var['value'], label='Actual')
    plt.plot(df_var['year'], df_var['predicted'], color='red', label='Regresión')
    plt.title(f'Regresión para {var} (Promedio)')
    plt.xlabel('Año')
    plt.ylabel(var)
    plt.legend()
    plt.show()

    print(f"Resumen del modelo para {var} (Promedio):\n", model.summary())

# Convertir las pendientes a un DataFrame para análisis
df_slopes = pd.DataFrame(list(slopes.items()), columns=['Variable', 'Pendiente'])

# Ordenar por velocidad de cambio
df_slopes = df_slopes.sort_values(by='Pendiente', ascending=False)

print("\nVariables con mayor aumento:")
print(df_slopes[df_slopes['Pendiente'] > 0].head())

print("\nVariables con mayor disminución:")
print(df_slopes[df_slopes['Pendiente'] < 0].tail())