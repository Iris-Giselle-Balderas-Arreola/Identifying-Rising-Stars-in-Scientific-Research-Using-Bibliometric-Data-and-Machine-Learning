# -*- coding: utf-8 -*-
"""Clusters

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yX9mvCdWhGd2nbd5X0rYNO28QprLNEVS
"""

from google.colab import drive
drive.mount('/content/drive')

#Librerías
import pandas as pd
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import numpy as np
from sklearn.cluster import KMeans

H_df = pd.read_csv('/content/drive/MyDrive/RETORising/main_database.csv' )

df = pd.read_csv('/content/drive/MyDrive/RETORising/metrics_disciplines_authors.csv')

# Asegurar que en H_df cada id tiene solo un valor único (ej. tomar el máximo HIndices_m-index_2022 si hay duplicados)
H_df_unique = H_df[['id', 'HIndices_m-index_2022']].drop_duplicates(subset='id', keep='first')

# Hacer el merge con df para tomar solo los id que están en df
h_m_df = df[['id']].merge(H_df_unique, on='id', how='inner')

# Mostrar información del nuevo DataFrame
print(h_m_df.shape)  # Debe ser menor o igual a df.shape
print(h_m_df.head())

# Seleccionar solo las columnas relevantes de H_df_unique (id + HIndices_h5-index_* columnas)
columns_to_add = ['id', 'HIndices_h5-index_2013', 'HIndices_h5-index_2014', 'HIndices_h5-index_2015',
                  'HIndices_h5-index_2016', 'HIndices_h5-index_2017', 'HIndices_h5-index_2018',
                  'HIndices_h5-index_2019', 'HIndices_h5-index_2020', 'HIndices_h5-index_2021',
                  'HIndices_h5-index_2022']

H_df = H_df.drop_duplicates(subset='id', keep='first')
# Asegurar que solo trabajamos con las columnas necesarias
H_df_subset = H_df[columns_to_add]
H_df_subset.fillna(0, inplace=True)  # Reemplazar NaN con 0

# Hacer el merge con df basado en la columna 'id'
df = df.merge(H_df_subset, on='id', how='left')  # 'left' para mantener todas las filas de df

df.info()

df.columns

# Suponiendo que tienes un DataFrame llamado df
# Crea el diccionario de mapeo con los números como claves y los nombres correspondientes como valores
column_mapping = {
    '10': 'Multidisciplinary',
    '11': 'Agricultural and Biological Sciences',
    '12': 'Arts and Humanities',
    '13': 'Biochemistry, Genetics and Molecular Biology',
    '14': 'Business, Management and Accounting',
    '15': 'Chemical Engineering',
    '16': 'Chemistry',
    '17': 'Computer Science',
    '18': 'Decision Sciences',
    '19': 'Earth and Planetary Sciences',
    '20': 'Economics, Econometrics and Finance',
    '21': 'Energy',
    '22': 'Engineering',
    '23': 'Environmental Science',
    '24': 'Immunology and Microbiology',
    '25': 'Materials Science',
    '26': 'Mathematics',
    '27': 'Medicine',
    '28': 'Neuroscience',
    '29': 'Nursing',
    '30': 'Pharmacology, Toxicology and Pharmaceutics',
    '31': 'Physics and Astronomy',
    '32': 'Psychology',
    '33': 'Social Sciences',
    '34': 'Veterinary',
    '35': 'Dentistry',
    '36': 'Health Professions'
}

# Renombra las columnas del DataFrame usando el diccionario
df.rename(columns=column_mapping, inplace=True)

# Suponiendo que df ya tiene las columnas renombradas con el diccionario column_mapping

# Creamos un diccionario para almacenar los subdataframes
subdataframes = {}

# Iteramos sobre cada columna (disciplina)
for columna in df.columns:
    # Filtramos las filas donde la columna tenga True (o 1)
    subdataframes[columna] = df[df[columna] == True].copy()

# Lista de columnas a eliminar (usando los nombres que se asignaron en el mapeo)
columnas_a_eliminar = list(column_mapping.values())

# Itera sobre cada subdataframe y elimina las columnas especificadas.
for key, subdf in subdataframes.items():
    subdataframes[key] = subdf.drop(columns=columnas_a_eliminar, errors='ignore')

# Ahora, cada subdataframe en el diccionario 'subdataframes' ya no tendrá las columnas indicadas.

# Ejemplo: acceder al subdataframe de 'Psychology'
subdataframes['Engineering']

# Diccionario para almacenar los subdataframes por disciplina y año
subdataframes_por_ano = {}

# Iterar sobre cada disciplina y su subdataframe
for disciplina, subdf in subdataframes.items():
    subdataframes_por_ano[disciplina] = {}  # Inicializamos el diccionario para la disciplina
    # Iterar sobre los años de interés (2013 a 2022)
    for year in range(2013, 2023):
        # Filtrar las columnas que contienen el sufijo '_{year}'
        columns_year = [col for col in subdf.columns if f"_{year}" in col]

        # Solo si se encontraron columnas para el año actual
        if columns_year:
            # Crear el DataFrame con la columna 'id' y las columnas del año actual
            subdataframes_por_ano[disciplina][year] = pd.concat(
                [subdf[['id']], subdf[columns_year]],
                axis=1
            )
        else:
            # Opcional: se puede asignar un DataFrame vacío o simplemente omitir este año
            subdataframes_por_ano[disciplina][year] = pd.DataFrame()

#Delimitar número de investigadores por área
areas =['Multidisciplinary','Agricultural and Biological Sciences','Arts and Humanities','Biochemistry, Genetics and Molecular Biology','Business, Management and Accounting',
    'Chemical Engineering','Chemistry','Computer Science','Decision Sciences','Earth and Planetary Sciences','Economics, Econometrics and Finance',
    'Energy','Engineering','Environmental Science','Immunology and Microbiology','Materials Science','Mathematics',
    'Medicine','Neuroscience','Nursing','Pharmacology, Toxicology and Pharmaceutics','Physics and Astronomy',
    'Psychology','Social Sciences','Veterinary','Dentistry','Health Professions']
for area in areas:
  researcher_count = df[area].eq(1).sum()
  print(f"Area: {area}, Number of Researchers: {researcher_count}")

plt.figure(figsize=(14, 7))
plt.bar(areas, [df[area].eq(1).sum() for area in areas])
plt.xlabel('Discipline')
plt.ylabel('Number of Researchers')
plt.title('Number of Researchers per Discipline')
plt.xticks(rotation=90, fontsize=10)  # Rotación en diagonal y fuente más pequeña
plt.show()

researcher_top_areas = df[areas].sum().sort_values(ascending=False)
researcher_top_areas.head(3)

#Creación de clusters para las tres áreas, a través de los años
#Área: Medicine
#Prueba clusterazo
X = subdataframes_por_ano['Medicine'][2013].iloc[:, 1:]
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
inercia = []
K_range = range(1, 10)
for k in K_range:
    kmeans = KMeans(n_clusters=k)
    kmeans.fit(X_scaled)
    inercia.append(kmeans.inertia_)

plt.figure(figsize=(8, 6))
plt.plot(K_range, inercia, marker='o')
plt.xlabel('Numer of Clusters for Medicine Category (k)')
plt.ylabel('Inertia')

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.linear_model import LinearRegression

# Crear un DataFrame vacío para almacenar los clusters históricos
cluster_history = pd.DataFrame()
centroids_history = {1: [], 2: []}  # Almacenar centroides de clusters 1 y 2
variables = None  # Se inicializará con los nombres de las variables extraídas del dataset
years = list(range(2013, 2023))

for year in years:
    # Select data for the current year and scale it
    X = subdataframes_por_ano['Medicine'][year].iloc[:, 1:]
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)

    # Perform KMeans clustering with 3 clusters
    kmeans = KMeans(n_clusters=3, random_state=42)
    kmeans.fit(X_scaled)
    subdataframes_por_ano['Medicine'][year]['cluster'] = kmeans.labels_

    # Centroides
    centroids = pd.DataFrame(kmeans.cluster_centers_, columns=X.columns)

    # Guardar la información de los clusters con el ID
    df_year = subdataframes_por_ano['Medicine'][year][['id']].copy()
    df_year[f'Cluster_{year}'] = subdataframes_por_ano['Medicine'][year]['cluster']

    # Combinar con el DataFrame histórico
    if cluster_history.empty:
        cluster_history = df_year
    else:
        cluster_history = cluster_history.merge(df_year, on='id', how='outer')

    # Create and display the box plot for H-index by cluster
    plt.figure(figsize=(8, 6))
    sns.boxplot(data=subdataframes_por_ano['Medicine'][year], x='cluster', y=f'HIndices_h5-index_{year}', palette='coolwarm')
    plt.xlabel('Cluster')
    plt.ylabel(f'H5-index ({year})')
    plt.title(f'Boxplot of H5-index by Cluster ({year})')
    plt.grid(axis='y', linestyle='--', alpha=0.7)
    plt.show()

    # Improved histogram to show only the three clusters with adjusted y-scale and count labels
    plt.figure(figsize=(8, 6))
    cluster_counts = subdataframes_por_ano['Medicine'][year]['cluster'].value_counts().sort_index()
    ax = sns.barplot(x=cluster_counts.index, y=cluster_counts.values, palette='coolwarm')
    plt.xlabel('Cluster')
    plt.ylabel('Count')
    plt.title(f'Number of Data Points per Cluster ({year})')
    plt.xticks(ticks=[0, 1, 2], labels=['Cluster 0', 'Cluster 1', 'Cluster 2'])
    plt.grid(axis='y', linestyle='--', alpha=0.7)

    for p in ax.patches:
        ax.annotate(f'{int(p.get_height())}', (p.get_x() + p.get_width() / 2., p.get_height()),
                    ha='center', va='bottom', fontsize=12, fontweight='bold', color='black')

    plt.ylim(0, cluster_counts.max() * 1.1)
    plt.show()

    # Heatmap to visualize centroid values
    plt.figure(figsize=(10, 6))
    sns.heatmap(centroids.T, cmap="coolwarm", annot=True, fmt=".2f")
    plt.xlabel("Cluster")
    plt.ylabel("Variables")
    plt.title(f"Values of the Variables in the Centroids ({year})")
    plt.show()

    # Guardar nombres de variables
    if variables is None:
        variables = list(X.columns)

    # Guardar centroides de clusters 1 y 2
    centroids_history[1].append(centroids.iloc[1].values)
    centroids_history[2].append(centroids.iloc[2].values)


# Convertir a DataFrame
df_centroids_1 = pd.DataFrame(centroids_history[1], index=years, columns=variables)
df_centroids_2 = pd.DataFrame(centroids_history[2], index=years, columns=variables)

# Graficar la regresión de los centroides para cada variable
fig, axes = plt.subplots(len(variables), 2, figsize=(12, 4 * len(variables)))

for i, var in enumerate(variables):
    for cluster, df_centroids, col in zip([1, 2], [df_centroids_1, df_centroids_2], [0, 1]):
        X = np.array(years).reshape(-1, 1)
        y = df_centroids[var].values
        model = LinearRegression()
        model.fit(X, y)
        y_pred = model.predict(X)

        ax = axes[i, col]
        ax.scatter(years, y, label=f'Cluster {cluster}', color='blue' if cluster == 1 else 'red', alpha=0.7)
        ax.plot(years, y_pred, linestyle='--', color='black', label='Regresión')
        ax.set_title(f'Evolución de {var} en Cluster {cluster}')
        ax.set_xlabel('Año')
        ax.set_ylabel(var)
        ax.legend()
        ax.grid(True, linestyle='--', alpha=0.5)

plt.tight_layout()
plt.show()

# Graficar la regresión de los centroides para cada variable
fig, axes = plt.subplots(len(variables), 2, figsize=(15, 5 * len(variables)))

for i, var in enumerate(variables):
    for cluster, df_centroids, col in zip([1, 2], [df_centroids_1, df_centroids_2], [0, 1]):
        X = np.array(years).reshape(-1, 1)
        y = df_centroids[var].values
        model = LinearRegression()
        model.fit(X, y)
        y_pred = model.predict(X)

        ax = axes[i, col]
        ax.scatter(years, y, label=f'Cluster {cluster}', color='blue' if cluster == 1 else 'red', alpha=0.7)
        ax.plot(years, y_pred, linestyle='--', color='black', label='Regression')
        ax.set_title(f'Evolution of {var} en Cluster {cluster}')
        ax.set_xlabel('Year')
        ax.set_ylabel(var)
        ax.legend()
        ax.grid(True, linestyle='--', alpha=0.5)

plt.tight_layout()
plt.show()

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression

# Función para validar claves de indexación
def safe_index(df, key):
    if isinstance(key, list) and not all(isinstance(k, (int, str)) for k in key):
        raise ValueError("Invalid index key: must be a list of integers or strings.")
    return df[key]

# Crear una tabla de transición de clusters
def cluster_transition_matrix(cluster_history):
    years = list(range(2013, 2023))
    transitions = []

    for i in range(len(years) - 1):
        prev_year = years[i]
        curr_year = years[i + 1]

        temp_df = cluster_history[['id', f'Cluster_{prev_year}', f'Cluster_{curr_year}']].dropna()

        for cluster in range(3):
            total_ids = (temp_df[f'Cluster_{prev_year}'] == cluster).sum()

            if total_ids > 0:
                for new_cluster in range(3):
                    transitioned = ((temp_df[f'Cluster_{prev_year}'] == cluster) &
                                    (temp_df[f'Cluster_{curr_year}'] == new_cluster)).sum()
                    percentage = (transitioned / total_ids) * 100
                    transitions.append([prev_year, curr_year, cluster, new_cluster, percentage])

    transition_df = pd.DataFrame(transitions, columns=['Prev_Year', 'Curr_Year', 'From_Cluster', 'To_Cluster', 'Percentage'])
    return transition_df

# Crear un gráfico de transición de clusters
def plot_cluster_transitions(table_transitions):
    plt.figure(figsize=(10, 6))
    pivot_df = table_transitions.pivot(index=['Prev_Year', 'From_Cluster'], columns='To_Cluster', values='Percentage').fillna(0)

    sns.heatmap(pivot_df, cmap='coolwarm', annot=True, fmt='.1f', linewidths=0.5)
    plt.xlabel('Destination cluster')
    plt.ylabel('Year and Cluster origin')
    plt.title('Cluster Transition Matrix')
    plt.show()



# Generar la tabla de transición
table_transitions = cluster_transition_matrix(cluster_history)
print(table_transitions)

# Graficar la matriz de transición
plot_cluster_transitions(table_transitions)

import pandas as pd

# Filtrar los IDs que pertenecen al clúster 1
def get_cluster_1_ids(subdataframes_por_ano, year):
    df = subdataframes_por_ano['Medicine'][year]
    ids_cluster_1 = df[df["cluster"] == 1]["id"].tolist()
    return ids_cluster_1

# Ejemplo de uso para un año específico
year = 2022
ids_cluster_1_2022 = get_cluster_1_ids(subdataframes_por_ano, year)
print(f"IDs en el cluster 1 para {year}:", ids_cluster_1_2022)
print(f"Total de IDs en el cluster 1 para {year}:", len(ids_cluster_1_2022))

ids_cluster_1_2013=get_cluster_1_ids(subdataframes_por_ano, 2013)

ids_1_2022_no_2013=list(set(ids_cluster_1_2022)-set(ids_cluster_1_2013))

print(ids_cluster_1_2022)

print(ids_1_2022_no_2013)

import pandas as pd
import numpy as np
import statsmodels.api as sm
import matplotlib.pyplot as plt

# Suponiendo que df ya está cargado y contiene los datos

df_original = df.copy()  # Hacer una copia del DataFrame original

# Convertimos el DataFrame a formato largo
id_column = 'id'
df_long = df.melt(id_vars=[id_column], var_name='variable', value_name='value')

# Extraer el año de las variables
# Suponemos que los nombres siguen un patrón como 'FieldWeightedCitationImpact_2013'
df_long['year'] = df_long['variable'].str.extract(r'(\d{4})').astype(float)
df_long['variable'] = df_long['variable'].str.replace(r'_\d{4}', '', regex=True)

# Lista de IDs a considerar
selected_ids = ids_1_2022_no_2013  # Reemplaza con los IDs que quieras analizar

# Filtrar el DataFrame para incluir solo los IDs seleccionados
df_long = df_long[df_long[id_column].isin(selected_ids)]

# Agrupar por variable y año para obtener la media
df_mean = df_long.groupby(['variable', 'year'])['value'].mean().reset_index()

# Diccionario para almacenar las pendientes de cada variable
slopes = {}

# Modelo de regresión para cada variable
for var in df_mean['variable'].unique():
    df_var = df_mean[df_mean['variable'] == var]

    X = sm.add_constant(df_var['year'])  # Agregar intercepto
    y = df_var['value']
    model = sm.OLS(y, X).fit()

    # Almacenar la pendiente (coeficiente del año)
    slope = model.params[1]
    slopes[var] = slope

    # Predicciones
    df_var['predicted'] = model.predict(X)

    # Visualización
    plt.figure()
    plt.scatter(df_var['year'], df_var['value'], label='Actual')
    plt.plot(df_var['year'], df_var['predicted'], color='red', label='Regression')
    plt.title(f'Regression for {var} (Mean)')
    plt.xlabel('Year')
    plt.ylabel(var)
    plt.legend()
    plt.show()

    print(f"Model summary for {var} (Mean):\n", model.summary())

# Convertir las pendientes a un DataFrame para análisis
df_slopes = pd.DataFrame(list(slopes.items()), columns=['Variable', 'Slope'])

# Ordenar por velocidad de cambio
df_slopes = df_slopes.sort_values(by='Slope', ascending=False)

print("\nVariables with the greatest increase:")
print(df_slopes[df_slopes['Slope'] > 0].head())

print("\nVariables with the greatest decrease:")
print(df_slopes[df_slopes['Slope'] < 0].tail())

len(ids_1_2022_no_2013)

print(ids_1_2022_no_2013)